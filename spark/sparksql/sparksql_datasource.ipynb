{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "'''\n",
    "来自：examples/src/main/python/sql/datasource.py\n",
    "包含：\n",
    "    1、读取、写入数据（三种方式）\n",
    "    2、根据 parquet file 、json file 等创建临时视图\n",
    "    3、合并 parquet schema\n",
    "    3、连接 mysql\n",
    "                    \n",
    "DataFrame函数：http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "以下示例均单机测试\n",
    "'''\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"datasource\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n",
      "+------+--------------+----------------+\n|  name|favorite_color|favorite_numbers|\n+------+--------------+----------------+\n|Alyssa|          null|  [3, 9, 15, 20]|\n|   Ben|           red|              []|\n+------+--------------+----------------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 三种读取方式\n",
    "\n",
    "df1 = spark.read.load(\"data/users.parquet\")\n",
    "df2 = spark.read.parquet(\"data/users.parquet\")\n",
    "df3 = spark.sql(\"select * from parquet.`data/users.parquet`\")\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 三种写入方式  当前pyspark==2.4.4\n",
    "\n",
    "df1.write.save(\"data/ouput1\",format=\"json\")\n",
    "df1.write.partitionBy(\"name\").format(\"json\").save(\"data/ouput2\")\n",
    "df1.write.json(\"data/ouput3\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 读取json，写入到parquet\n",
    "\n",
    "dfj = spark.read.load(\"data/people.json\", format=\"json\")\n",
    "dfj.select(\"name\", \"age\")\\\n",
    "    .write\\\n",
    "    .save(\"data/namesAndAges.parquet\", format=\"parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "root\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n",
      "+------+\n|  name|\n+------+\n|Justin|\n+------+\n\n",
      "+----------------+----+\n|         address|name|\n+----------------+----+\n|[Columbus, Ohio]| Yin|\n+----------------+----+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 读取json的另一种形式： RDD\n",
    "\n",
    "def json_dataset_example(spark):\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "# path：string represents path to the JSON dataset,or RDD of Strings storing JSON objects.\n",
    "\n",
    "    path = \"data/people.json\"\n",
    "    peopleDF = spark.read.json(path)\n",
    "    peopleDF.printSchema()\n",
    "    \n",
    "    peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "    teenagerNamesDF.show()\n",
    "\n",
    "    # Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "    # an RDD[String] storing one JSON object per string\n",
    "    jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "    otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "    otherPeople = spark.read.json(otherPeopleRDD)\n",
    "    otherPeople.show()\n",
    "\n",
    "json_dataset_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+------------------+\n|      name;age;job|\n+------------------+\n|Jorge;30;Developer|\n|  Bob;32;Developer|\n+------------------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 读取csv\n",
    "\n",
    "dfc = spark.read.load(\"data/people.csv\",\n",
    "                      format=\"csv\", \n",
    "                      sep=\":\", \n",
    "                      inferSchema=\"true\", \n",
    "                      header=\"true\")\n",
    "dfc.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# 读取orc，写入到orc\n",
    "\n",
    "dfo = spark.read.orc(\"data/users.orc\")\n",
    "dfo.write.format(\"orc\")\\\n",
    "    .option(\"orc.bloom.filter.columns\", \"favorite_color\")\\\n",
    "    .option(\"orc.dictionary.key.threshold\", \"1.0\")\\\n",
    "    .save(\"data/users_with_options.orc\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+------+\n|  name|\n+------+\n|Justin|\n+------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 读取json文件，写入到parquet\n",
    "# 再根据 parquet file 创建临时视图\n",
    "\n",
    "def parquet_example(spark):\n",
    "    peopleDF = spark.read.json(\"data/people.json\")\n",
    "    peopleDF.write.parquet(\"data/people.parquet\")\n",
    "    \n",
    "    parquetFile = spark.read.parquet(\"data/people.parquet\")\n",
    "    \n",
    "    parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "    \n",
    "    spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\").show()\n",
    "\n",
    "parquet_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# 合并 parquet schema\n",
    "\n",
    "from pyspark.sql import Row\n",
    "def parquet_schema_merging_example(spark):\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    squaresDF = spark.createDataFrame(sc.parallelize(range(1,6))\n",
    "                                      .map(lambda x:Row(single=x,double=x**2)))\n",
    "    squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "    \n",
    "    cubesDF = spark.createDataFrame(sc.parallelize(range(6, 11))\n",
    "                                    .map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "    cubesDF.write.parquet(\"data/test_table/key=2\")\n",
    "    \n",
    "    mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_table\")\n",
    "    mergedDF.printSchema()\n",
    "    mergedDF.show()\n",
    "\n",
    "parquet_schema_merging_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取avro的数据，写入到avro\n",
    "\n",
    "从 MAVEN 下载对应版本的 JAR 包 `spark-avro_2.12-3.0.1.jar`，放到 Spark 的 `jars` 目录下 【要和spark版本对应】\n",
    "\n",
    "集群执行：`spark-submit --master spark://zgg:7077 avro_read_sparksql.py`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"datasource_avro\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "dfa = spark.read.format(\"avro\").load(\"file:///root/data/users.avro\")\n",
    "\n",
    "dfa.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"file:///root/data/namesAndFavColors.avro\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "1. 启动kafka生产者和消费者\n",
    "2. 集群执行：spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1 \n",
    "                         --master spark://zgg:7077\n",
    "                         avro_kafka_sparksql.py  \n",
    "                         \n",
    "生产者发送avros数据：https://www.cnblogs.com/fangjb/archive/2004/01/13/13355086.html\n",
    "'''\n",
    "\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"datasource_avro\")\\\n",
    "    .getOrCreate()\n",
    "# `from_avro` requires Avro schema in JSON string format.\n",
    "jsonFormatSchema = open(\"/root/data/user.avsc\", \"r\").read()\n",
    "\n",
    "df = spark\\\n",
    "  .readStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"zgg:9092\")\\\n",
    "  .option(\"subscribe\", \"avrofrom\")\\\n",
    "  .load()\n",
    "\n",
    "# 1. Decode the Avro data into a struct;\n",
    "# 2. Filter by column `favorite_color`;\n",
    "# 3. Encode the column `name` in Avro format.\n",
    "output = df\\\n",
    "  .select(from_avro(\"value\", jsonFormatSchema).alias(\"user\"))\\\n",
    "  .where('user.favorite_color == \"red\"')\\\n",
    "  .select(to_avro(\"user.name\").alias(\"value\"))\n",
    "\n",
    "query = output\\\n",
    "  .writeStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"zgg:9092\")\\\n",
    "  .option(\"topic\", \"avroto\")\\\n",
    "  .option(\"checkpointLocation\",\"/opt/spark-3.0.1-bin-hadoop3.2/checkpointLocation\")\\\n",
    "  .start()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 连接 JDBC source\n",
    "\n",
    "要将 `mysql-connector-java-8.0.21.jar` 放到 spark 的 `jars` 目录下\n",
    "\n",
    "集群执行：`spark-submit --master spark://zgg:7077 mysql_integration__sparksql_test.py`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test_spark_mysql.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"datasource\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "def jdbc_dataset_example(spark):\n",
    "    # Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods\n",
    "    # Loading data from a JDBC source\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306\") \\\n",
    "        .option(\"dbtable\", \"mysql.dept_emp\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .load()\n",
    "\n",
    "    jdbcDF.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306\") \\\n",
    "        .option(\"dbtable\", \"mysql.dept_emp_bk\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .save()\n",
    "\n",
    "jdbc_dataset_example(spark)  \n",
    "\n",
    "# 更多连接方式见 examples/src/main/python/sql/datasource.py"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 连接 hive\n",
    "\n",
    "1、在 classpath 添加许多依赖，将 hive-site.xml, core-site.xml 和 hdfs-site.xml 放到 conf/. 目录下。\n",
    "\n",
    "2、实例化一个 Hive 支持的 SparkSession\n",
    "\n",
    "3、集群执行：`spark-submit --master spark://zgg:7077 hive_integration_sparksql_test.py >hive_test_spark.log`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def test_hive_example():\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
    "    spark.sql(\"LOAD DATA LOCAL INPATH '/root/data/kv1.txt' INTO TABLE src\")\n",
    "    \n",
    "    print(\"SHOW ALL DATA:\")\n",
    "    spark.sql(\"SELECT * FROM src\").show()\n",
    "    \n",
    "    print(\"AGGREGATION QUERIES:\")\n",
    "# Aggregation queries are also supported.\n",
    "    spark.sql(\"SELECT COUNT(*) FROM src\").show()\n",
    "\n",
    "    print(\"WHERE FILTER DATA:\")\n",
    "# The results of SQL queries are themselves DataFrames and support all normal functions.\n",
    "    sqlDF = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n",
    "\n",
    "    print(\"TRANSFER TO RDD:\")\n",
    "# The items in DataFrames are of type Row, which allows you to access each column by ordinal.\n",
    "    stringsDS = sqlDF.rdd.map(lambda row: \"Key: %d, Value: %s\" % (row.key, row.value))\n",
    "    for record in stringsDS.collect():\n",
    "        print(record)\n",
    "        \n",
    "# You can also use DataFrames to create temporary views within a SparkSession.\n",
    "    Record = Row(\"key\", \"value\")\n",
    "    recordsDF = spark.createDataFrame([Record(i, \"val_\" + str(i)) for i in range(1, 101)])\n",
    "    recordsDF.createOrReplaceTempView(\"records\")\n",
    "\n",
    "    print(\"CREATE TEMP VIEW:\")\n",
    "# Queries can then join DataFrame data with data stored in Hive.\n",
    "    spark.sql(\"SELECT * FROM records r JOIN src s ON r.key = s.key\").show()\n",
    "\n",
    "test_hive_example()\n",
    "\n",
    "# 写入到持久化表\n",
    "# df1.write.partitionBy(\"favorite_color\")\\\n",
    "#     .bucketBy(42,\"name\")\\\n",
    "#     .saveAsTable(\"people_partitioned_bucketed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 一些通用配置\n",
    "\n",
    "下面内容仅支持spark3.x。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+-------------+\n|         file|\n+-------------+\n|file1.parquet|\n|file2.parquet|\n+-------------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 忽略损坏文件\n",
    "\n",
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "# dir1/file3.json is corrupt from parquet's view\n",
    "test_corrupt_df = spark.read.parquet(\"data/dir1\",\"data/dir1/dir2\")\n",
    "test_corrupt_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+-------------+\n|         file|\n+-------------+\n|file1.parquet|\n|file2.parquet|\n+-------------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 忽略缺失文件\n",
    "\n",
    "spark.sql(\"set spark.sql.files.ignoreMissingFiles=true\")\n",
    "test_corrupt_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+-------------+\n|         file|\n+-------------+\n|file1.parquet|\n+-------------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 路径全局过滤\n",
    "\n",
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=false\")\n",
    "test_filter_df = spark.read.load(\"data/dir1\",format=\"parquet\",pathGlobFilter=\"*.parquet\")\n",
    "test_filter_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+-------------+\n|         file|\n+-------------+\n|file1.parquet|\n|file2.parquet|\n+-------------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 递归查找文件\n",
    "\n",
    "spark.sql(\"set spark.sql.files.ignoreCorruptFiles=true\")\n",
    "recursive_loaded_df = spark.read.format(\"parquet\")\\\n",
    "    .option(\"recursiveFileLookup\",\"true\")\\\n",
    "    .load(\"data/dir1\")\n",
    "recursive_loaded_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 表间join\n",
    "\n",
    "详细描述见性能调优页面\n",
    "\n",
    "```sh\n",
    ">>> spark.sql(\"select * from test\").show()\n",
    "+----+------+----+\n",
    "|dept|userid| sal|\n",
    "+----+------+----+\n",
    "|  d1| user1|1000|\n",
    "|  d1| user2|2000|\n",
    "|  d1| user3|3000|\n",
    "|  d2| user4|4000|\n",
    "|  d2| user5|5000|\n",
    "+----+------+----+\n",
    "\n",
    ">>> spark.table(\"test\").join(spark.table(\"test\").hint(\"broadcast\"), \"userid\").show()\n",
    "+------+----+----+----+----+                                                    \n",
    "|userid|dept| sal|dept| sal|\n",
    "+------+----+----+----+----+\n",
    "| user1|  d1|1000|  d1|1000|\n",
    "| user2|  d1|2000|  d1|2000|\n",
    "| user3|  d1|3000|  d1|3000|\n",
    "| user4|  d2|4000|  d2|4000|\n",
    "| user5|  d2|5000|  d2|5000|\n",
    "+------+----+----+----+----+\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}