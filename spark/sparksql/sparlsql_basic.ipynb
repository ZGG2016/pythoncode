{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\nroot\n |-- age: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "'''\n",
    "来自：examples/src/main/python/sql/basic.py\n",
    "包含：\n",
    "    1、DataFrame的基本使用、创建视图、执行sql语句\n",
    "    2、RDD和Datasets互相转换：\n",
    "                    （1）转换一个 Row 对象的 RDD 成一个 DataFrame。\n",
    "                    （2）StructType\n",
    "                    \n",
    "DataFrame函数：http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "以下示例均单机测试\n",
    "'''\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"basicfunc\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.json(\"data/people.json\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+----+\n| age|\n+----+\n|null|\n|  30|\n|  19|\n+----+\n\n",
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n",
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n",
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  31|   Andy|\n|  20| Justin|\n+----+-------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df.select(\"age\").show()\n",
    "df.select(\"age\",\"name\").show()\n",
    "df.select(df[\"age\"],df[\"name\"]).show()\n",
    "df.select((df[\"age\"]+1).alias(\"age\"),df[\"name\"]).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+---+----+\n|age|name|\n+---+----+\n| 30|Andy|\n+---+----+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df.filter(df[\"age\"]>20).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+----+-----+\n| age|count|\n+----+-----+\n|  19|    1|\n|null|    1|\n|  30|    1|\n+----+-----+\n\n",
      "+----+-----+\n| age|count|\n+----+-----+\n|  19|    1|\n|null|    1|\n|  30|    1|\n+----+-----+\n\n",
      "+----+-------+-----+\n| age|   name|count|\n+----+-------+-----+\n|null|Michael|    1|\n|  30|   Andy|    1|\n|  19| Justin|    1|\n+----+-------+-----+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df.groupBy(df[\"age\"]).count().show()\n",
    "df.groupBy(\"age\").count().show()\n",
    "df.groupBy(\"age\",\"name\").count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"peoplev\")\n",
    "spark.sql(\"select * from peoplev\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"peoplevgl\")\n",
    "df.createGlobalTempView(\"peoplevgl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n+----+-------+\n| age|   name|\n+----+-------+\n|null|Michael|\n|  30|   Andy|\n|  19| Justin|\n+----+-------+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.peoplevgl\").show()\n",
    "spark.newSession().sql(\"select * from global_temp.peoplevgl\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "name:Justin\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "'''\n",
    "schema推断\n",
    "'''\n",
    "def schema_inference_example(spark):\n",
    "    sc = spark.sparkContext  # s是小写\n",
    "    data = sc.textFile(\"data/people.txt\")\n",
    "    parts = data.map(lambda x:x.split(\",\"))\n",
    "    people = parts.map(lambda x:Row(name = x[0],age = int(x[1])))  # Row\n",
    "    \n",
    "    schemaPeople = spark.createDataFrame(people)\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "    teenagers = spark.sql(\"select name from people where age >=13 and age <=19\")\n",
    "    \n",
    "    teenNames = teenagers.rdd.map(lambda x:\"name:\"+x.name).collect()\n",
    "    for name in teenNames:\n",
    "        print(name)\n",
    "    \n",
    "schema_inference_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "+-------+----+\n|    age|name|\n+-------+----+\n|Michael|  29|\n|   Andy|  30|\n| Justin|  19|\n+-------+----+\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StringType, StructType\n",
    "'''\n",
    "schema推断\n",
    "'''\n",
    "def programmatic_schema_example(spark):\n",
    "    sc = spark.sparkContext\n",
    "    data = sc.textFile(\"data/people.txt\")\n",
    "    parts = data.map(lambda x:x.split(\",\"))\n",
    "    people = parts.map(lambda x:(x[0],x[1].strip()))\n",
    "\n",
    "    schemaString = \"age name\"\n",
    "    fields = [StructField(filed_name,StringType(),True) for filed_name in schemaString.split(\" \")]\n",
    "    schema = StructType(fields)\n",
    "    \n",
    "    schemaPeople = spark.createDataFrame(people,schema)\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "    \n",
    "    spark.sql(\"select * from people\").show()\n",
    "\n",
    "programmatic_schema_example(spark)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}